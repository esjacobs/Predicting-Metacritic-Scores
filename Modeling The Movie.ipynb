{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: The Movie\n",
    "\n",
    "(Go to the READ.ME of this repository for the entire write-up.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modeling, we took the practice of throwing everything at the wall and seeing what worked. We imported many different models, including linear regression, lasso, SGD regressor, bagging regressor, random forrest regressor, SVR, and adaboost regressor, as well as classifiers including logistic regression, random forest classifier, adaboost classifier, k-nearest neighbors classifier, decision tree classifier, and even a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imdb\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, SGDRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We brought in our six dataframes:\n",
    "1. 1 df 2 = directors and actors weighted, , deleted columns with 1 or fewer terms\n",
    "2. 2 df 2 = directors and actors weighted, deleted columns with 1 or fewer terms\n",
    "3. 3 df 2 = directors and actors and writers weighted, deleted columns with 1 or fewer terms\n",
    "4. 1 df 3 = directors and actors weighted, , deleted columns with 2 or fewer terms\n",
    "5. 2 df 2 = directors and actors weighted, deleted columns with 2 or fewer terms\n",
    "6. 3 df 2 = directors and actors and writers weighted, deleted columns with 2 or fewer terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-made dataframes with directors weighted\n",
    "\n",
    "# X_train = pd.read_csv('train_everything_director_weights_df2.csv') # 1\n",
    "# X_test = pd.read_csv('test_everything_director_weights_df2.csv') # 1\n",
    "# X_train = pd.read_csv('train_everything_director_actor_weights_df2.csv') # 2\n",
    "# X_test = pd.read_csv('test_everything_director_actor_weights_df2.csv') # 2 \n",
    "X_train = pd.read_csv('train_everything_director_actor_writer_weights_df2.csv') # 3\n",
    "X_test = pd.read_csv('test_everything_director_actor_writer_weights_df2.csv') # 3\n",
    "# X_train = pd.read_csv('train_everything_director_weights_df3.csv') # 4\n",
    "# X_test = pd.read_csv('test_everything_director_weights_df3.csv') # 4\n",
    "# X_train = pd.read_csv('train_everything_director_actor_weights_df3.csv') # 5\n",
    "# X_test = pd.read_csv('test_everything_director_actor_weights_df3.csv') # 5\n",
    "# X_train = pd.read_csv('train_everything_director_actor_writer_weights_df3.csv') # 6\n",
    "# X_test = pd.read_csv('test_everything_director_actor_writer_weights_df3.csv') # 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fed the dataframes through the following cell, which gave us three regressor scores, then transformed our y variable for classification (based on median Metacritic score) and fed that through three classifiers. Throughout this process many models were attempted and thrown out. Dataframes were changed and had to be saved again and reloaded. At the end of the day we decided on the following models:\n",
    "\n",
    "- Regression\n",
    "    - Bagging Regressor\n",
    "    - Random Forest Regressor\n",
    "    - LASSO\n",
    "- Classification\n",
    "    - Logistic Regression\n",
    "    - Bagging Classifier\n",
    "    - Random Forest Classifier\n",
    "    \n",
    "Except for LASSO and logistic regression, there wasn't much rhyme or reason for modeling choices. These just gave us the best relative scores (of the ones we tried), and also didn't take a huge amount of time. Also, the bagging regressor and classifier, which didn't seem to ever give us scores that were as good as the other models, still worked quickly and served as a veritable canary in a coal mine, warning us if something had gone wrong with the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br test score: \n",
      "0.03739589734130877\n",
      "\n",
      "rf test score: \n",
      "-0.02765041735558893\n",
      "\n",
      "lasso test score: \n",
      "0.21460320119560503\n",
      "\n",
      "logreg test score: \n",
      "0.6854026845637584\n",
      "\n",
      "br test score: \n",
      "0.6593959731543624\n",
      "\n",
      "rf test score: \n",
      "0.6753355704697986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = X_train.Metascore\n",
    "y_test = X_test.Metascore\n",
    "\n",
    "X_train.drop(['Metascore'], axis=1, inplace=True)\n",
    "X_test.drop(['Metascore'], axis=1, inplace=True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "br = BaggingRegressor()\n",
    "br.fit(X_train, y_train)\n",
    "# print('br train score: ')\n",
    "# print(br.score(X_train, y_train))\n",
    "print('br test score: ')\n",
    "print(br.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "# print('rf train score: ')\n",
    "# print(rf.score(X_train, y_train))\n",
    "print('rf test score: ')\n",
    "print(rf.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "lasso = Lasso(.15)\n",
    "lasso.fit(X_train, y_train)\n",
    "# print('rf train score: ')\n",
    "# print(rf.score(X_train, y_train))\n",
    "print('lasso test score: ')\n",
    "print(lasso.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "median = np.median(y_train)\n",
    "\n",
    "new_y = []\n",
    "for n in y_train:\n",
    "    if n > median:\n",
    "        new_y.append(1)\n",
    "    else:\n",
    "        new_y.append(0)\n",
    "y_train = new_y\n",
    "\n",
    "new_y = []\n",
    "for n in y_test:\n",
    "    if n > median:\n",
    "        new_y.append(1)\n",
    "    else:\n",
    "        new_y.append(0)\n",
    "y_test = new_y\n",
    "\n",
    "logreg = LogisticRegression() \n",
    "logreg.fit(X_train, y_train)\n",
    "# print('logreg train score: ')\n",
    "# print(logreg.score(X_train, y_train))\n",
    "print('logreg test score: ')\n",
    "print(logreg.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "br = BaggingClassifier()\n",
    "br.fit(X_train, y_train)\n",
    "# print('br train score: ')\n",
    "# print(br.score(X_train, y_train))\n",
    "print('br test score: ')\n",
    "print(br.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "# print('rf train score: ')\n",
    "# print(rf.score(X_train, y_train))\n",
    "print('rf test score: ')\n",
    "print(rf.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "# 1 reg \n",
    "\n",
    "# br test score: \n",
    "# 0.038342711196289625\n",
    "\n",
    "# rf test score: \n",
    "# 0.11832620794676674\n",
    "\n",
    "# lasso test score: \n",
    "# 0.19244316790430385\n",
    "\n",
    "# 1 class \n",
    "\n",
    "# logreg test score: \n",
    "# 0.6736577181208053\n",
    "\n",
    "# br test score: \n",
    "# 0.662751677852349\n",
    "\n",
    "# rf test score: \n",
    "# 0.6753355704697986\n",
    "\n",
    "# 2 reg \n",
    "\n",
    "# br test score: \n",
    "# 0.006896130293002622\n",
    "\n",
    "# rf test score: \n",
    "# 0.07139091002869702\n",
    "\n",
    "# lasso test score: \n",
    "# 0.1924431679043039\n",
    "\n",
    "# 2 class\n",
    "\n",
    "# logreg test score: \n",
    "# 0.6736577181208053\n",
    "\n",
    "# br test score: \n",
    "# 0.6375838926174496\n",
    "\n",
    "# rf test score: \n",
    "# 0.6585570469798657\n",
    "\n",
    "# 3 reg\n",
    "\n",
    "# br test score: \n",
    "# 0.05994540328342234\n",
    "\n",
    "# rf test score: \n",
    "# -0.03186605837286138\n",
    "\n",
    "# lasso test score: \n",
    "# 0.1924431679043039\n",
    "\n",
    "# 3 class\n",
    "\n",
    "# logreg test score: \n",
    "# 0.6736577181208053\n",
    "\n",
    "# br test score: \n",
    "# 0.6384228187919463\n",
    "\n",
    "# rf test score: \n",
    "# 0.6719798657718121\n",
    "\n",
    "# 4 reg\n",
    "\n",
    "# br test score: \n",
    "# 0.023266042810753954\n",
    "\n",
    "# rf test score: \n",
    "# 0.07619378931494514\n",
    "\n",
    "# lasso test score: \n",
    "# 0.21460320119560472\n",
    "\n",
    "# 4 class \n",
    "\n",
    "# logreg test score: \n",
    "# 0.6854026845637584\n",
    "\n",
    "# br test score: \n",
    "# 0.6434563758389261\n",
    "\n",
    "# rf test score: \n",
    "# 0.6375838926174496\n",
    "\n",
    "# 5 reg\n",
    "\n",
    "# br test score: \n",
    "# 0.005276011558945859\n",
    "\n",
    "# rf test score: \n",
    "# 0.03497975713168888\n",
    "\n",
    "# lasso test score: \n",
    "# 0.21460320119560497\n",
    "\n",
    "# 5 class \n",
    "\n",
    "# logreg test score: \n",
    "# 0.6854026845637584\n",
    "\n",
    "# br test score: \n",
    "# 0.6518456375838926\n",
    "\n",
    "# rf test score: \n",
    "# 0.6610738255033557\n",
    "\n",
    "# 6 reg\n",
    "\n",
    "# br test score: \n",
    "# 0.03739589734130877\n",
    "\n",
    "# rf test score: \n",
    "# -0.02765041735558893\n",
    "\n",
    "# lasso test score: \n",
    "# 0.21460320119560503\n",
    "\n",
    "# 6 class\n",
    "\n",
    "# logreg test score: \n",
    "# 0.6854026845637584\n",
    "\n",
    "# br test score: \n",
    "# 0.6593959731543624\n",
    "\n",
    "# rf test score: \n",
    "# 0.6753355704697986\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_mods = pd.read_csv('capstone_models_1.csv')\n",
    "\n",
    "cap_mods.columns\n",
    "\n",
    "cap_mods.columns = ['', '1 df 3', '2 df 3', '3 df 3', '1 df 2', '2 df2 ',\n",
    "       '3 df 2 ']\n",
    "\n",
    "cap_mods = cap_mods.set_index('')\n",
    "\n",
    "cap_mods_class = cap_mods.iloc[3:,:].copy()\n",
    "\n",
    "cap_mods_reg = cap_mods.iloc[:3,:].copy()\n",
    "\n",
    "cap_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\",{\"xtick.color\":\"black\", \"ytick.color\":\"black\"})\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(cap_mods_reg, annot = True, cmap=\"Greens\")\n",
    "# plt.tick_params(color='white', labelcolor='white');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"dark\",{\"xtick.color\":\"white\", \"ytick.color\":\"white\"})\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.heatmap(cap_mods_class, annot = True, cmap = \"Blues\")\n",
    "# plt.tick_params(color='white', labelcolor='white');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1 df 3</th>\n",
       "      <th>2 df 3</th>\n",
       "      <th>3 df 3</th>\n",
       "      <th>1 df 2</th>\n",
       "      <th>2 df2</th>\n",
       "      <th>3 df 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br reg</td>\n",
       "      <td>0.038343</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.005276</td>\n",
       "      <td>0.037396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf reg</td>\n",
       "      <td>0.118326</td>\n",
       "      <td>0.071391</td>\n",
       "      <td>-0.031866</td>\n",
       "      <td>0.076194</td>\n",
       "      <td>0.034980</td>\n",
       "      <td>-0.027650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lasso reg</td>\n",
       "      <td>0.192443</td>\n",
       "      <td>0.192443</td>\n",
       "      <td>0.192443</td>\n",
       "      <td>0.214603</td>\n",
       "      <td>0.214603</td>\n",
       "      <td>0.214603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg class</td>\n",
       "      <td>0.673658</td>\n",
       "      <td>0.673658</td>\n",
       "      <td>0.673658</td>\n",
       "      <td>0.685403</td>\n",
       "      <td>0.685403</td>\n",
       "      <td>0.685403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>br class</td>\n",
       "      <td>0.662752</td>\n",
       "      <td>0.637584</td>\n",
       "      <td>0.638423</td>\n",
       "      <td>0.643456</td>\n",
       "      <td>0.651846</td>\n",
       "      <td>0.659396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf class</td>\n",
       "      <td>0.675336</td>\n",
       "      <td>0.658557</td>\n",
       "      <td>0.671980</td>\n",
       "      <td>0.637584</td>\n",
       "      <td>0.661074</td>\n",
       "      <td>0.675336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    1 df 3    2 df 3    3 df 3    1 df 2    2 df2    3 df 2 \n",
       "0        br reg  0.038343  0.006896  0.059945  0.023266  0.005276  0.037396\n",
       "1        rf reg  0.118326  0.071391 -0.031866  0.076194  0.034980 -0.027650\n",
       "2     lasso reg  0.192443  0.192443  0.192443  0.214603  0.214603  0.214603\n",
       "3  logreg class  0.673658  0.673658  0.673658  0.685403  0.685403  0.685403\n",
       "4      br class  0.662752  0.637584  0.638423  0.643456  0.651846  0.659396\n",
       "5      rf class  0.675336  0.658557  0.671980  0.637584  0.661074  0.675336"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = X_train.Metascore\n",
    "# y_test = X_test.Metascore\n",
    "\n",
    "# X_train.drop(['Metascore'], axis=1, inplace=True)\n",
    "# X_test.drop(['Metascore'], axis=1, inplace=True)\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# X_train = ss.fit_transform(X_train)\n",
    "# X_test = ss.transform(X_test)\n",
    "\n",
    "# median = np.median(y_train)\n",
    "\n",
    "# new_y = []\n",
    "# for n in y_train:\n",
    "#     if n > median:\n",
    "#         new_y.append(1)\n",
    "#     else:\n",
    "#         new_y.append(0)\n",
    "# y_train = new_y\n",
    "\n",
    "# new_y = []\n",
    "# for n in y_test:\n",
    "#     if n > median:\n",
    "#         new_y.append(1)\n",
    "#     else:\n",
    "#         new_y.append(0)\n",
    "# y_test = new_y\n",
    "\n",
    "rf_params = {\n",
    "    'max_depth': [None, 1000],\n",
    "    'n_estimators': [200],\n",
    "    'max_features': [2, 10],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf, param_grid=rf_params)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.score(X_test, y_test))\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "# 0.7030201342281879\n",
    "# 0.667\n",
    "# {'max_depth': None, 'max_features': 10, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "\n",
    "Your Capstone project is the culmination of your time at GA. You will be tasked with developing an interesting question, collecting the data required to model that data, developing the strongest model (or models) for prediction, and communicating those findings to other data scientists and non-technical individuals. This introductory document lays out the five consitutent portions of the project and their due dates.\n",
    "\n",
    "## Your Deliverables\n",
    "\n",
    "- A well-made predictive model using either structured or unstructured machine learning techniques (or other technique approved in advanced by the global instructors), as well as clean, well-written code. \n",
    "- A technical report aimed at fellow data scientists that explains your process and findings\n",
    "- A public presentation of your findings aimed at laypeople. \n",
    "\n",
    "### **[Capstone, Part 1: Topic Proposals](./part_01/)**\n",
    "\n",
    "In Part 1, get started by choosing **three potential topics and problems**, describing your goals & criteria for success, potential audience(s), and identifying 1-2 potential datasets. In the field of data science, good projects are practical. Your capstone project should be manageable and affect a real world audience. This might be a domain you are familiar with, a particular interest you have, something that affects a community you are involved in, or an area that relates to a field you wish to work in.\n",
    "\n",
    "One of the best ways to test ideas quickly is to share them with others. A good data scientist has to be comfortable discussing ideas and presenting to audiences. That's why for Part 1 of your Capstone project, you'll be preparing a lightning talk in addition to your initial notebook outlining the scope of your project.  You will present your candidate topics in a slide deck, and should be prepared to answer questions and defend your data selection(s). Presentations should take no more than 3-5 minutes.\n",
    "\n",
    "**The ultimate choice of topic for your capstone project is yours!** However, this is research and development work. Sometimes projects that look easy can be difficult and vice versa. It never hurts to have a second (or third) option available.\n",
    "\n",
    "- **Goal**: Prepare a 3-5 minute lightning talk that covers three potential topics, including potential sources of data, goals, metrics and audience.\n",
    "- **Due**: Thursday, June 7\n",
    "\n",
    "### **[Capstone, Part 2: Problem Statement + EDA](./part_02/)**\n",
    "\n",
    "For Part 2, provide a clear statement of the problem that you have chosen and an overview of your approach to solving that problem. Summarize your objectives, goals & success metrics, and any risks & assumptions. Outline your proposed methods and models, perform your initial EDA, and summarize the process. **Your data should be in hand by this point in the process!**\n",
    "\n",
    "**Again, your data should be in hand by this point the process!**\n",
    "\n",
    "- **Goal**: Describe your proposed approach and summarize your initial EDA in a code submission to your local instructor ([submission link](https://docs.google.com/forms/d/e/1FAIpQLScez-8PsyIgP548fNtsoDpuNTdKxsr6tVvKPDtbr-mQov6NCw/viewform?usp=sf_link))\n",
    "- **Due**: Wednesday, June 20\n",
    "\n",
    "### **[Capstone, Part 3: Progress Report + Preliminary Findings](./part_03/)**\n",
    "\n",
    "In Part 3, you'll create a progress report of your work in order to get feedback along the way. Describe your approach, initial results, and any setbacks or lessons learned so far. Your report should include updated visual and statistical analysis of your data. You’ll also meet with your local instructional team to get feedback on your results so far!\n",
    "\n",
    "- **Goal**: Discuss progress and setbacks, include visual and statistical analysis, review with instructor. (A submission link for your progress report will be provided prior to the due date.)\n",
    "- **Due**: Monday, July 2\n",
    "\n",
    "### **[Capstone, Part 4: Report Writeup + Technical Analysis](./part_04/)**\n",
    "\n",
    "By now, you're ready to apply your modeling skills to make machine learning predictions. Your goal for Part 4 is to develop a technical document (in the form of Jupyter notebook) that can be shared among your peers.\n",
    "\n",
    "Document your research and analysis including a summary, an explanation of your modeling approach as well as the strengths and weaknesses of any variables in the process. You should provide insight into your analysis, using best practices like cross validation or applicable prediction metrics.\n",
    "\n",
    "- **Goal**: Detailed report and code with a summary of your statistical analysis, model, and evaluation metrics.\n",
    "- **Due**: Friday, July 13\n",
    "\n",
    "### **[Capstone, Part 5: Presentation + Recommendations](./part_05/)**\n",
    "\n",
    "Whether during an interview or as part of a job, you will frequently have to present your findings to business partners and other interested parties - many of whom won't know anything about data science! That's why for Part 5, you'll create a presentation of your previous findings with a non-technical audience in mind.\n",
    "\n",
    "You should already have the analytical work complete, so now it's time to clean up and clarify your findings. Come up with a detailed slide deck or interactive demo that explains your data, visualizes your model, describes your approach, articulates strengths and weaknesses, and presents specific recommendations. Be prepared to explain and defend your model to an inquisitive audience!\n",
    "\n",
    "- **Goal**: Detailed presentation deck that relates your data, model, and findings to a non-technical audience.\n",
    "- **Due**: Tuesday, July 17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
